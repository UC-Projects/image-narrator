{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "083d1e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2 \n",
    "import os\n",
    "import glob\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a0f1a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('dict_variables.pkl', 'rb') as file:\n",
    "    dict_variables = pickle.load(file)\n",
    "with open('train_variables.pkl', 'rb') as file:\n",
    "    train_variables = pickle.load(file)\n",
    "with open('val_variables.pkl', 'rb') as file:\n",
    "    val_variables = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d87a03ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(train_variables['X_train'])\n",
    "y_in_train = np.array(train_variables['y_in_train'], dtype='float64')\n",
    "y_out_train = np.array(train_variables['y_out_train'], dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab78bb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = np.array(val_variables['X_val'])\n",
    "y_in_val = np.array(val_variables['y_in_val'], dtype='float64')\n",
    "y_out_val = np.array(val_variables['y_out_val'], dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7b299f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_dict = dict_variables['captions_dict']\n",
    "new_dict = dict_variables['new_dict']\n",
    "images_features = dict_variables['images_features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef57c728",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "MAX_LEN = 0\n",
    "for k, vv in captions_dict.items():\n",
    "    for v in vv:\n",
    "        if len(v) > MAX_LEN:\n",
    "            MAX_LEN = len(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7d034e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import concatenate\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dense, Flatten,Input, Convolution2D, Dropout, LSTM, TimeDistributed, Embedding, Bidirectional, Activation, RepeatVector,Concatenate\n",
    "from keras.models import Sequential, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3b95288",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " embedding_input (InputLaye  [(None, 84)]                 0         []                            \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " dense_input (InputLayer)    [(None, 2048)]               0         []                            \n",
      "                                                                                                  \n",
      " embedding (Embedding)       (None, 84, 128)              953600    ['embedding_input[0][0]']     \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 128)                  262272    ['dense_input[0][0]']         \n",
      "                                                                                                  \n",
      " lstm (LSTM)                 (None, 84, 256)              394240    ['embedding[0][0]']           \n",
      "                                                                                                  \n",
      " repeat_vector (RepeatVecto  (None, 84, 128)              0         ['dense[0][0]']               \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " time_distributed (TimeDist  (None, 84, 128)              32896     ['lstm[0][0]']                \n",
      " ributed)                                                                                         \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 84, 256)              0         ['repeat_vector[0][0]',       \n",
      "                                                                     'time_distributed[0][0]']    \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)               (None, 84, 256)              525312    ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization (Batch  (None, 84, 256)              1024      ['lstm_1[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 84, 256)              0         ['batch_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " lstm_2 (LSTM)               (None, 84, 128)              197120    ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_1 (Bat  (None, 84, 128)              512       ['lstm_2[0][0]']              \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)         (None, 84, 128)              0         ['batch_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " lstm_3 (LSTM)               (None, 512)                  1312768   ['dropout_1[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_2 (Bat  (None, 512)                  2048      ['lstm_3[0][0]']              \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)         (None, 512)                  0         ['batch_normalization_2[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 512)                  262656    ['dropout_2[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_3 (Bat  (None, 512)                  2048      ['dense_2[0][0]']             \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)         (None, 512)                  0         ['batch_normalization_3[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (None, 256)                  131328    ['dropout_3[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_4 (Bat  (None, 256)                  1024      ['dense_3[0][0]']             \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)         (None, 256)                  0         ['batch_normalization_4[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dense_4 (Dense)             (None, 7450)                 1914650   ['dropout_4[0][0]']           \n",
      "                                                                                                  \n",
      " activation (Activation)     (None, 7450)                 0         ['dense_4[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5993498 (22.86 MB)\n",
      "Trainable params: 5990170 (22.85 MB)\n",
      "Non-trainable params: 3328 (13.00 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, Input, RepeatVector, TimeDistributed, Activation,Dropout, BatchNormalization, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "# Define constants\n",
    "embedding_size = 128\n",
    "max_len = MAX_LEN\n",
    "vocab_size = len(new_dict)\n",
    "\n",
    "# Define image model\n",
    "image_model = Sequential([\n",
    "    Dense(embedding_size, input_shape=(2048,), activation='relu'),\n",
    "    RepeatVector(max_len)\n",
    "])\n",
    "\n",
    "# Define language model\n",
    "language_model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_size, input_length=max_len),\n",
    "    LSTM(256, return_sequences=True),\n",
    "    TimeDistributed(Dense(embedding_size))\n",
    "])\n",
    "\n",
    "# Concatenate image and language models\n",
    "conca = Concatenate()([image_model.output, language_model.output])\n",
    "\n",
    "x = LSTM(256, return_sequences=True)(conca)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "x = LSTM(128, return_sequences=True)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "# Global LSTM layer\n",
    "x = LSTM(512, return_sequences=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "# Dense layers\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "# Output layer\n",
    "x = Dense(vocab_size)(x)\n",
    "out = Activation('softmax')(x)\n",
    "\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=[image_model.input, language_model.input], outputs=out)\n",
    "\n",
    "# Compile the model\n",
    "optimizer = RMSprop(learning_rate=0.001)  # You can adjust the learning rate\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55aab58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150028, 2048) (150028, 84) (150028, 7450)\n",
      "(37508, 2048) (37508, 84) (37508, 7450)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape,y_in_train.shape,y_out_train.shape)\n",
    "print(X_val.shape,y_in_val.shape,y_out_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb1a6da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "4689/4689 [==============================] - 1941s 414ms/step - loss: 5.9450 - accuracy: 0.1107 - val_loss: 5.8575 - val_accuracy: 0.1137\n",
      "Epoch 2/100\n",
      "4689/4689 [==============================] - 1828s 390ms/step - loss: 5.9188 - accuracy: 0.1114 - val_loss: 5.8140 - val_accuracy: 0.1137\n",
      "Epoch 3/100\n",
      "4689/4689 [==============================] - 1903s 406ms/step - loss: 5.8438 - accuracy: 0.1114 - val_loss: 5.7653 - val_accuracy: 0.1137\n",
      "Epoch 4/100\n",
      "4689/4689 [==============================] - 1842s 393ms/step - loss: 5.7602 - accuracy: 0.1117 - val_loss: 5.7144 - val_accuracy: 0.1137\n",
      "Epoch 5/100\n",
      "4689/4689 [==============================] - 1799s 384ms/step - loss: 5.7219 - accuracy: 0.1118 - val_loss: 5.6938 - val_accuracy: 0.1137\n",
      "Epoch 6/100\n",
      "4689/4689 [==============================] - 1835s 391ms/step - loss: 5.7266 - accuracy: 0.1116 - val_loss: 5.6933 - val_accuracy: 0.1137\n",
      "Epoch 7/100\n",
      "4689/4689 [==============================] - 1852s 395ms/step - loss: 5.6977 - accuracy: 0.1169 - val_loss: 5.4235 - val_accuracy: 0.1675\n",
      "Epoch 8/100\n",
      "4689/4689 [==============================] - 1819s 388ms/step - loss: 5.4452 - accuracy: 0.1756 - val_loss: 5.3133 - val_accuracy: 0.1964\n",
      "Epoch 9/100\n",
      "4689/4689 [==============================] - 1921s 410ms/step - loss: 5.3632 - accuracy: 0.1913 - val_loss: 5.1318 - val_accuracy: 0.2152\n",
      "Epoch 10/100\n",
      "4689/4689 [==============================] - 1862s 397ms/step - loss: 5.1704 - accuracy: 0.2222 - val_loss: 5.0061 - val_accuracy: 0.2374\n",
      "Epoch 11/100\n",
      "4689/4689 [==============================] - 1894s 404ms/step - loss: 5.1241 - accuracy: 0.2286 - val_loss: 4.9711 - val_accuracy: 0.2356\n",
      "Epoch 12/100\n",
      "4689/4689 [==============================] - 1876s 400ms/step - loss: 5.0983 - accuracy: 0.2323 - val_loss: 4.9499 - val_accuracy: 0.2450\n",
      "Epoch 13/100\n",
      "4689/4689 [==============================] - 1895s 404ms/step - loss: 5.0835 - accuracy: 0.2363 - val_loss: 4.8958 - val_accuracy: 0.2506\n",
      "Epoch 14/100\n",
      "4689/4689 [==============================] - 1934s 412ms/step - loss: 5.0392 - accuracy: 0.2476 - val_loss: 4.8339 - val_accuracy: 0.2644\n",
      "Epoch 15/100\n",
      "4689/4689 [==============================] - 1944s 415ms/step - loss: 4.9946 - accuracy: 0.2565 - val_loss: 4.8303 - val_accuracy: 0.2671\n",
      "Epoch 16/100\n",
      "4689/4689 [==============================] - 1842s 393ms/step - loss: 4.9633 - accuracy: 0.2607 - val_loss: 4.7358 - val_accuracy: 0.2743\n",
      "Epoch 17/100\n",
      "4689/4689 [==============================] - 1814s 387ms/step - loss: 4.9432 - accuracy: 0.2635 - val_loss: 4.7491 - val_accuracy: 0.2713\n",
      "Epoch 18/100\n",
      "4689/4689 [==============================] - 1816s 387ms/step - loss: 4.9222 - accuracy: 0.2676 - val_loss: 4.7534 - val_accuracy: 0.2745\n",
      "Epoch 19/100\n",
      "4689/4689 [==============================] - 1852s 395ms/step - loss: 4.9082 - accuracy: 0.2701 - val_loss: 4.6681 - val_accuracy: 0.2772\n",
      "Epoch 20/100\n",
      "4689/4689 [==============================] - 1832s 391ms/step - loss: 4.8894 - accuracy: 0.2719 - val_loss: 4.7306 - val_accuracy: 0.2783\n",
      "Epoch 21/100\n",
      "4689/4689 [==============================] - 1837s 392ms/step - loss: 4.8812 - accuracy: 0.2755 - val_loss: 4.6891 - val_accuracy: 0.2808\n",
      "Epoch 22/100\n",
      "4689/4689 [==============================] - 1832s 391ms/step - loss: 4.8704 - accuracy: 0.2770 - val_loss: 4.7566 - val_accuracy: 0.2764\n",
      "Epoch 23/100\n",
      "4689/4689 [==============================] - 1807s 385ms/step - loss: 4.8575 - accuracy: 0.2791 - val_loss: 4.6660 - val_accuracy: 0.2926\n",
      "Epoch 24/100\n",
      "4689/4689 [==============================] - 1799s 384ms/step - loss: 4.8483 - accuracy: 0.2791 - val_loss: 4.6584 - val_accuracy: 0.2912\n",
      "Epoch 25/100\n",
      "4689/4689 [==============================] - 1839s 392ms/step - loss: 4.8482 - accuracy: 0.2788 - val_loss: 4.6417 - val_accuracy: 0.2928\n",
      "Epoch 26/100\n",
      "4689/4689 [==============================] - 1829s 390ms/step - loss: 4.8425 - accuracy: 0.2803 - val_loss: 4.6480 - val_accuracy: 0.2873\n",
      "Epoch 27/100\n",
      "4689/4689 [==============================] - 1818s 388ms/step - loss: 4.8296 - accuracy: 0.2799 - val_loss: 4.6346 - val_accuracy: 0.2930\n",
      "Epoch 28/100\n",
      "4689/4689 [==============================] - 1826s 389ms/step - loss: 4.8221 - accuracy: 0.2814 - val_loss: 4.6493 - val_accuracy: 0.2911\n",
      "Epoch 29/100\n",
      "4689/4689 [==============================] - 1831s 391ms/step - loss: 4.8209 - accuracy: 0.2817 - val_loss: 4.6380 - val_accuracy: 0.2918\n",
      "Epoch 30/100\n",
      "4689/4689 [==============================] - 1854s 395ms/step - loss: 4.8244 - accuracy: 0.2814 - val_loss: 4.6315 - val_accuracy: 0.2914\n",
      "Epoch 31/100\n",
      "4689/4689 [==============================] - 1845s 393ms/step - loss: 4.8131 - accuracy: 0.2818 - val_loss: 4.6182 - val_accuracy: 0.2950\n",
      "Epoch 32/100\n",
      "4689/4689 [==============================] - 1884s 402ms/step - loss: 4.8162 - accuracy: 0.2818 - val_loss: 4.6563 - val_accuracy: 0.2938\n",
      "Epoch 33/100\n",
      "4689/4689 [==============================] - 1906s 406ms/step - loss: 4.8108 - accuracy: 0.2831 - val_loss: 4.6334 - val_accuracy: 0.2948\n",
      "Epoch 34/100\n",
      "4689/4689 [==============================] - 1993s 425ms/step - loss: 4.8087 - accuracy: 0.2831 - val_loss: 4.6784 - val_accuracy: 0.2927\n",
      "Epoch 35/100\n",
      "4689/4689 [==============================] - 1903s 406ms/step - loss: 4.8117 - accuracy: 0.2822 - val_loss: 4.6291 - val_accuracy: 0.2922\n",
      "Epoch 36/100\n",
      "4689/4689 [==============================] - 1893s 404ms/step - loss: 4.8087 - accuracy: 0.2827 - val_loss: 4.6452 - val_accuracy: 0.2875\n",
      "Epoch 37/100\n",
      "4689/4689 [==============================] - 1848s 394ms/step - loss: 4.8053 - accuracy: 0.2825 - val_loss: 4.6210 - val_accuracy: 0.2926\n",
      "Epoch 38/100\n",
      "4689/4689 [==============================] - 1863s 397ms/step - loss: 4.8004 - accuracy: 0.2833 - val_loss: 4.7524 - val_accuracy: 0.2774\n",
      "Epoch 39/100\n",
      "4689/4689 [==============================] - 1926s 411ms/step - loss: 4.8076 - accuracy: 0.2813 - val_loss: 4.6608 - val_accuracy: 0.2852\n",
      "Epoch 40/100\n",
      "4689/4689 [==============================] - 2021s 431ms/step - loss: 4.8093 - accuracy: 0.2821 - val_loss: 4.6409 - val_accuracy: 0.2931\n",
      "Epoch 41/100\n",
      "4689/4689 [==============================] - 2040s 435ms/step - loss: 4.8129 - accuracy: 0.2821 - val_loss: 4.6458 - val_accuracy: 0.2936\n",
      "Epoch 42/100\n",
      "4689/4689 [==============================] - 2004s 427ms/step - loss: 4.7941 - accuracy: 0.2836 - val_loss: 4.6446 - val_accuracy: 0.2896\n",
      "Epoch 43/100\n",
      "4689/4689 [==============================] - 1952s 416ms/step - loss: 4.7949 - accuracy: 0.2837 - val_loss: 4.6331 - val_accuracy: 0.2986\n",
      "Epoch 44/100\n",
      "4689/4689 [==============================] - 1943s 414ms/step - loss: 4.7985 - accuracy: 0.2844 - val_loss: 4.6448 - val_accuracy: 0.2958\n",
      "Epoch 45/100\n",
      "4689/4689 [==============================] - 1953s 416ms/step - loss: 4.7967 - accuracy: 0.2842 - val_loss: 4.6675 - val_accuracy: 0.2940\n",
      "Epoch 46/100\n",
      "4689/4689 [==============================] - 1900s 405ms/step - loss: 4.7945 - accuracy: 0.2848 - val_loss: 4.6596 - val_accuracy: 0.2966\n",
      "Epoch 47/100\n",
      "4689/4689 [==============================] - 1882s 401ms/step - loss: 4.7939 - accuracy: 0.2848 - val_loss: 4.6126 - val_accuracy: 0.2956\n",
      "Epoch 48/100\n",
      "4689/4689 [==============================] - 1879s 401ms/step - loss: 4.7924 - accuracy: 0.2840 - val_loss: 4.6183 - val_accuracy: 0.3006\n",
      "Epoch 49/100\n",
      "4689/4689 [==============================] - 1863s 397ms/step - loss: 4.7835 - accuracy: 0.2848 - val_loss: 4.6211 - val_accuracy: 0.2956\n",
      "Epoch 50/100\n",
      "4689/4689 [==============================] - 1958s 418ms/step - loss: 4.7822 - accuracy: 0.2831 - val_loss: 4.6277 - val_accuracy: 0.2970\n",
      "Epoch 51/100\n",
      "4689/4689 [==============================] - 1988s 424ms/step - loss: 4.7926 - accuracy: 0.2850 - val_loss: 4.6334 - val_accuracy: 0.2991\n",
      "Epoch 52/100\n",
      "4689/4689 [==============================] - 1974s 421ms/step - loss: 4.7972 - accuracy: 0.2835 - val_loss: 4.6747 - val_accuracy: 0.2896\n",
      "Epoch 53/100\n",
      "4689/4689 [==============================] - 1942s 414ms/step - loss: 4.7943 - accuracy: 0.2836 - val_loss: 4.6571 - val_accuracy: 0.2942\n",
      "Epoch 54/100\n",
      "4689/4689 [==============================] - 2380s 508ms/step - loss: 4.7992 - accuracy: 0.2832 - val_loss: 4.6525 - val_accuracy: 0.2905\n",
      "Epoch 55/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4689/4689 [==============================] - 2182s 465ms/step - loss: 4.8045 - accuracy: 0.2826 - val_loss: 4.6784 - val_accuracy: 0.2916\n",
      "Epoch 56/100\n",
      "4689/4689 [==============================] - 2011s 429ms/step - loss: 4.7966 - accuracy: 0.2840 - val_loss: 4.6741 - val_accuracy: 0.3017\n",
      "Epoch 57/100\n",
      "4689/4689 [==============================] - 2023s 431ms/step - loss: 4.7904 - accuracy: 0.2844 - val_loss: 4.6640 - val_accuracy: 0.2886\n",
      "Epoch 58/100\n",
      "4689/4689 [==============================] - 1869s 399ms/step - loss: 4.8050 - accuracy: 0.2817 - val_loss: 4.6707 - val_accuracy: 0.2949\n",
      "Epoch 59/100\n",
      "4689/4689 [==============================] - 1825s 389ms/step - loss: 4.8078 - accuracy: 0.2819 - val_loss: 4.6913 - val_accuracy: 0.2874\n",
      "Epoch 60/100\n",
      "4689/4689 [==============================] - 1830s 390ms/step - loss: 4.8054 - accuracy: 0.2817 - val_loss: 4.6845 - val_accuracy: 0.2940\n",
      "Epoch 61/100\n",
      "1021/4689 [=====>........................] - ETA: 23:05 - loss: 4.8149 - accuracy: 0.2815"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    [X_train, y_in_train], \n",
    "    y_out_train, \n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_data=([X_val, y_in_val], y_out_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60247f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_dict = {v:k for k, v in new_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454c7937",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('image_narrator_model.h5')\n",
    "model.save_weights('in_model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffe8c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(x):\n",
    "    test_img_path = images[x]\n",
    "    img = cv2.imread(test_img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (224,224))\n",
    "    test_img = img.reshape(1,224,224,3)\n",
    "    \n",
    "    return test_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894b7d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from tensorflow.keras.models import load_model\n",
    "images_path=\"flickr30k/Images/flickr30k_images/\"\n",
    "images=glob.glob(images_path+\"*.jpg\")\n",
    "\n",
    "loaded_model = load_model('image_narrator_model.h5')\n",
    "def generate_caption(image_index):\n",
    "    # Get image\n",
    "    test_feature = loaded_model.predict(get_image(image_index)).reshape(1,2048)\n",
    "    if test_feature is None:\n",
    "        return\n",
    "    \n",
    "    test_img_path = images[image_index]\n",
    "    test_img = cv2.imread(test_img_path)\n",
    "    test_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Initialize caption input\n",
    "    text_inp = ['startofseq']\n",
    "\n",
    "    # Generate caption\n",
    "    count = 0\n",
    "    caption = ''\n",
    "    while count < 25:\n",
    "        count += 1\n",
    "        encoded = [new_dict.get(word, new_dict['<OUT>']) for word in text_inp]\n",
    "        encoded = [encoded]\n",
    "        encoded = pad_sequences(encoded, padding='post', truncating='post', maxlen=MAX_LEN)\n",
    "        prediction = np.argmax(loaded_model.predict([test_feature, encoded]))\n",
    "        sampled_word = inv_dict[prediction]\n",
    "        caption = caption + ' ' + sampled_word\n",
    "        if sampled_word == 'endofseq':\n",
    "            break\n",
    "        text_inp.append(sampled_word)\n",
    "\n",
    "    # Display image and caption using OpenCV\n",
    "    cv2.imshow('image', test_img)\n",
    "    print('Caption:', caption)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
